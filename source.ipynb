{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import joblib\n",
    "from scipy.sparse import csr_matrix, save_npz, load_npz\n",
    "import zlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = (\"Ramnit\", \"Lollipop\", \"Kelihos_ver3\", \"Vundo\", \"Simda\", \"Tracur\", \"Kelihos_ver1\", \"Obfuscator.ACY\", \"Gatak\")\n",
    "TRAIN_DIRECTORY_PATH = \"../train/\"\n",
    "CSV_FEATURES_LOCATION = \"./CSVFeatures/\"\n",
    "OBJECT_DUMP_PATH = \"./ObjectDump/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadData:\n",
    "    @staticmethod\n",
    "    def readBytes(fileID):\n",
    "        with (TRAIN_DIRECTORY_PATH + fileID + \".bytes\").open() as file:\n",
    "            data = file.read()\n",
    "        contents = data.split()\n",
    "        byteList = []\n",
    "        for item in contents:\n",
    "            if len(item) == 2 and item != \"??\":\n",
    "                byteList.append(item)\n",
    "        return byteList\n",
    "    \n",
    "    @staticmethod\n",
    "    def readAsmAsString(fileID):\n",
    "        with (TRAIN_DIRECTORY_PATH + fileID + \".asm\").open(encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "            data = file.read()\n",
    "        return data\n",
    "    \n",
    "    @staticmethod\n",
    "    def readAsmAsList(fileID):\n",
    "        with (TRAIN_DIRECTORY_PATH + fileID + \".asm\").open(encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "            return file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVFeatures:\n",
    "    def __init__(self,fileName):\n",
    "        self.fileName = fileName\n",
    "    def save(self, data):\n",
    "        data.to_csv(CSV_FEATURES_LOCATION + self.fileName)\n",
    "    def load(self):\n",
    "        return pd.read_csv(CSV_FEATURES_LOCATION + self.fileName).set_index(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpzFeature:\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "\n",
    "    def load(self):\n",
    "        path = OBJECT_DUMP_PATH + self.file\n",
    "        return pd.DataFrame(load_npz(path).toarray()) if path.exists() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class List:\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "\n",
    "    def save(self, data):\n",
    "        path = OBJECT_DUMP_PATH + self.file\n",
    "        with path.open(\"w\", encoding=\"utf-8\") as file:\n",
    "            for line in data:\n",
    "                file.write(f\"{str(line)}\\n\")\n",
    "\n",
    "    def load(self):\n",
    "        path = OBJECT_DUMP_PATH + self.file\n",
    "        if path.exists():\n",
    "            with path.open(encoding=\"utf-8\") as file:\n",
    "                return file.read().splitlines(\"\\n\")\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractNGrams(ids, sequenceReader, n):\n",
    "    class Reader:\n",
    "        def __init__(self, id):\n",
    "            self._id = id\n",
    "\n",
    "        def read(self):\n",
    "            return sequenceReader(self._id)\n",
    "\n",
    "    sequences = [Reader(id) for id in ids]\n",
    "    # Name mangling should be considered, `token_pattern` cannot be the default.\n",
    "    ngrmVector = CountVectorizer(ngram_range=(n, n), stop_words=None, token_pattern=r\"(?u)\\b[\\w@?]{2,}\\b\", lowercase=False, input=\"file\")\n",
    "    ngrms = ngrmVector.fit_transform(sequences)\n",
    "    return ngrmVector, ngrms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\n",
      "3    2942\n",
      "2    2478\n",
      "1    1541\n",
      "8    1228\n",
      "9    1013\n",
      "6     751\n",
      "4     475\n",
      "7     398\n",
      "5      42\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "TRAIN_LABELS = pd.read_csv(\"../Analysis-Kaggle-Work/trainLabels.csv\")\n",
    "TRAIN_LABELS[\"ID\"] = TRAIN_LABELS[\"ID\"].astype(str)\n",
    "TRAIN_LABELS[\"Class\"] = TRAIN_LABELS[\"Class\"].astype(\"category\")\n",
    "TRAIN_LABELS.set_index(\"ID\", inplace=True)\n",
    "print(TRAIN_LABELS.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classCounts = TRAIN_LABELS['Class'].value_counts().sort_index()\n",
    "ax = sns.barplot(x=classCounts.index, y=classCounts.values)\n",
    "ax.set(xlabel=\"Class\", ylabel=\"Number of Samples\", title=\"Class Distribution Bar Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE # 1 => File Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFileSizes(fileIDs):\n",
    "    df = pd.DataFrame(columns=[\"ID\", \"Asm-Size\", \"Byte-Size\", \"Ratio\"], dtype=float).set_index(\"ID\")\n",
    "    for id in tqdm(fileIDs):\n",
    "        df.at[id, \"Asm-Size\"] = os.path.getsize(TRAIN_DIRECTORY_PATH + id + \".asm\")\n",
    "        df.at[id, \"Byte-Size\"] = os.path.getsize(TRAIN_DIRECTORY_PATH + id + \".bytes\")\n",
    "    df[[\"Asm-Size\", \"Byte-Size\"]] = df[[\"Asm-Size\", \"Byte-Size\"]].astype(int)\n",
    "    df[\"Ratio\"] = (df[\"Asm-Size\"] / df[\"Byte-Size\"]).round(5)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileSizes = extractFileSizes(TRAIN_LABELS.index)\n",
    "CSVFeatures(\"fileSizes.csv\").save(fileSizes)\n",
    "fileSizes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of File Size Ratio based on Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "xLimit = (0, fileSizes[\"Asm-Size\"].mean() + fileSizes[\"Asm-Size\"].std() * 3)\n",
    "yLimit = (0, fileSizes[\"Byte-Size\"].mean() + fileSizes[\"Byte-Size\"].std() * 3)\n",
    "ax = sns.scatterplot(x=\"Asm-Size\", y=\"Byte-Size\", hue=\"Class\", data=fileSizes.assign(Class=TRAIN_LABELS[\"Class\"]))    \n",
    "ax.set(xlim=xLimit, ylim=yLimit, xlabel=\"Disassembly Size\", ylabel=\"Machine Code Size\", title=\"Distribution Of File Sizes\")\n",
    "ax.legend(loc=\"upper right\", title=\"Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features # 2 => Section Attributions\n",
    "\n",
    "Extract section attributions from `.asm` files, including:\n",
    "\n",
    "|     Size     |                         Description                          |\n",
    "| :----------: | :----------------------------------------------------------: |\n",
    "| Virtual Size |    The total size of the section when it is loaded into memory. |\n",
    "|   Raw Size   | The size of the section or the size of the initialized data in the disk file. |\n",
    "|    Flags     |              Executable, readable and writable.              |\n",
    "\n",
    "Section attributions have the following pattern:\n",
    "\n",
    "```\n",
    ".text:00401000         ; Section 1. (virtual address 00001000)\n",
    ".text:00401000         ; Virtual size             : 0002964D ( 169549.)\n",
    ".text:00401000         ; Section size in file     : 00029800 ( 169984.)\n",
    ".text:00401000         ; Offset to raw data for section: 00000400\n",
    ".text:00401000         ; Flags 60000020: Text Executable Readable\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectionAttributesRegex = re.compile(r\"([A-Za-z]+):[\\dA-F]{8}\\s+;\\s+Virtual\\s+size\\s+:\\s+[\\dA-F]{8}\\s+\\(\\s*(\\d+)\\.[\\s\\S]+?Section\\s+size\\s+in\\s+file\\s+:\\s+[\\dA-F]{8}\\s+\\(\\s*(\\d+)\\.[\\s\\S]+?;\\s+Flags\\s+[\\dA-F]{8}:[\\s\\S]+?\\s+((?:Executable|Readable|Writable|\\s)+)\", flags=re.IGNORECASE)\n",
    "# Test Drive\n",
    "sectionAttributesRegex.findall(ReadData.readAsmAsString(TRAIN_LABELS.index[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Section:\n",
    "    def __init__(self):\n",
    "        self.name = \"\"\n",
    "        self.virtualSize = 0\n",
    "        self.rawSize = 0\n",
    "        self.executable = False\n",
    "        self.writable = False\n",
    "\n",
    "    @staticmethod\n",
    "    def load(id):\n",
    "        asm = ReadData.asm(id)\n",
    "        sections = []\n",
    "        for attribute in sectionAttributesRegex.findall(asm):\n",
    "            section = Section()\n",
    "            section.name, section.virtualSize, section.rawSize, access = attribute[0].lower(), int(attribute[1]), int(attribute[2]), set(attribute[3].split())\n",
    "            if \"Executable\" in access:\n",
    "                section.executable = True\n",
    "            if \"Writable\" in access:\n",
    "                section.writable = True\n",
    "            sections.append(section)\n",
    "        return sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two dataframes for section attributions:\n",
    "\n",
    "  - The *1st* saves each section's sizes.\n",
    "  - The *2nd* saves the total sizes of sections grouped by access properties.\n",
    "\n",
    "For each section or access property `<X>`, there will be three columns named `<X>-Virtual`, `<X>-Raw` and `<X>-Ratio` storing its virtual size, raw size and size ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _saveBySections(sectionDF, uniqueSectionIntegerColumns, fileID, attributes):\n",
    "        sectionDF.at[fileID, f\"{attributes.name}-Virtual\"] = attributes.virtualSize\n",
    "        sectionDF.at[fileID, f\"{attributes.name}-Raw\",] = attributes.rawSize\n",
    "        sectionDF.at[fileID, f\"{attributes.name}-Ratio\",] = round(attributes.rawSize / attributes.virtualSize, 5)\n",
    "        uniqueSectionIntegerColumns.update([f\"{attributes.name}-Virtual\", f\"{attributes.name}-Raw\"])\n",
    "        return sectionDF, uniqueSectionIntegerColumns\n",
    "    \n",
    "def _saveByAccess(readWriteExecuteDF, fileID, attributes):\n",
    "    readWriteExecuteDF.at[fileID, \"Readable-Virtual\"] += attributes.virtualSize\n",
    "    readWriteExecuteDF.at[fileID, \"Readable-Raw\"] += attributes.rawSize\n",
    "    if attributes.writable:\n",
    "        readWriteExecuteDF.at[fileID, \"Writable-Virtual\"] += attributes.virtualSize\n",
    "        readWriteExecuteDF.at[fileID, \"Writable-Raw\"] += attributes.rawSize\n",
    "    if attributes.executable:\n",
    "        readWriteExecuteDF.at[fileID, \"Executable-Virtual\"] += attributes.virtualSize\n",
    "        readWriteExecuteDF.at[fileID, \"Executable-Raw\"] += attributes.rawSize\n",
    "\n",
    "def extractSectionAttributes(fileIDs):\n",
    "    readWriteExecuteColumns = [str(i)+\"-\"+str(j) for i in [\"Executable\", \"Writable\", \"Readable\"] for j in [\"Virtual\", \"Raw\", \"Ratio\"]]\n",
    "    readWriteExecuteDF = pd.DataFrame(columns=[\"ID\"] + readWriteExecuteColumns, dtype=float).set_index(\"ID\")\n",
    "    sectionDF = pd.DataFrame(columns=[\"ID\"], dtype=float).set_index(\"ID\")\n",
    "    uniqueSectionIntegerColumns = set()\n",
    "    for fileID in tqdm(fileIDs):\n",
    "        readWriteExecuteDF.loc[fileID, :] = 0\n",
    "        sectionDF.loc[fileID, :] = 0\n",
    "        for attribute in Section.load(fileID):\n",
    "            _saveBySections(sectionDF, uniqueSectionIntegerColumns, fileID, attribute)\n",
    "            _saveByAccess(readWriteExecuteDF, fileID, attribute)\n",
    "\n",
    "    sectionDF.fillna(0, inplace=True)\n",
    "    uniqueSectionIntegerColumns = list(uniqueSectionIntegerColumns)\n",
    "    sectionDF[uniqueSectionIntegerColumns] = sectionDF[uniqueSectionIntegerColumns].astype(int)\n",
    "\n",
    "    readWriteExecuteDF.fillna(0, inplace=True)\n",
    "    readWriteExecuteIntColumns = [i for i in readWriteExecuteDF.columns if \"-Virtual\" in i or \"-Raw\" in i]\n",
    "    readWriteExecuteDF[readWriteExecuteIntColumns] = readWriteExecuteDF[readWriteExecuteIntColumns].astype(int)\n",
    "\n",
    "    readWriteExecuteDF[\"Readable-Ratio\"] = (readWriteExecuteDF[\"Readable-Raw\"] / readWriteExecuteDF[\"Readable-Virtual\"]).round(5)\n",
    "    readWriteExecuteDF[\"Writable-Ratio\"] = (readWriteExecuteDF[\"Writable-Raw\"] / readWriteExecuteDF[\"Writable-Virtual\"]).round(5)\n",
    "    readWriteExecuteDF[\"Executable-Ratio\"] = (readWriteExecuteDF[\"Executable-Raw\"] / readWriteExecuteDF[\"Executable-Virtual\"]).round(5)\n",
    "    return sectionDF, readWriteExecuteDF.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectionSizes, readWriteExecuteSizes = extractSectionAttributes(TRAIN_LABELS.index)\n",
    "\n",
    "CSVFeatures(\"sectionPermissions.csv\").save(readWriteExecuteSizes)\n",
    "sectionSizes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readWriteExecuteSizes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "#### Using Random Forests to pick the most important Section Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xTest, yTrain, yTest = train_test_split(sectionSizes, TRAIN_LABELS, test_size=0.2, stratify=TRAIN_LABELS)\n",
    "\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(xTrain, yTrain.values.ravel())\n",
    "predictions = classifier.predict(xTest)\n",
    "\n",
    "accuracy_score(yTest, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topFeaturesIndex = np.argsort(classifier.feature_importances_)[::-1][:25]\n",
    "topFeatures = xTrain.columns[topFeaturesIndex]\n",
    "\n",
    "topFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Features and their Importance Visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "ax = sns.barplot(x=classifier.feature_importances_[topFeaturesIndex], y=topFeatures)\n",
    "ax.set(xlabel=\"Importance\", ylabel=\"Feature\", title=\"Importance of Section Sizes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique feature list\n",
    "set([i[0] for i in topFeatures.str.split(\"-\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard sections are:\n",
    "\n",
    "|   Section   |          Description           |\n",
    "| :---------: | :----------------------------: |\n",
    "| text, code  |        Executable code         |\n",
    "|    data     |          Normal data           |\n",
    "|    idata    | Import libraries and functions |\n",
    "|    rdata    |         Read-only data         |\n",
    "|     bss     |     Block starting symbols     |\n",
    "|     tls     |     Thread Local Storage       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topSectionSizes = sectionSizes[topFeatures]\n",
    "topSectionSizes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSVFeatures(\"sectionSizes.csv\").save(topSectionSizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE # 3 => API 4-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, extract function calls from `.asm` files. The pattern looks like:\n",
    "\n",
    "```asm\n",
    "call    _memmove_s\n",
    "call    ds:VirtualAllocEx\n",
    "call    sub_6D1757D6\n",
    "```\n",
    "\n",
    "The prefix `ds` should be removed from a function name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functionCallsRegex = re.compile(r\"\\scall\\s+(?:ds:)(?:__imp_)?([^\\s]+)\", flags=re.IGNORECASE)\n",
    "functionCallsRegex.findall(ReadData.readAsmAsString(TRAIN_LABELS.index[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But not all these calls are related to system functions. Usually, system functions are imported by the Import Table. Their patterns are;\n",
    "\n",
    "```\n",
    "extrn    LoadResource:dword\n",
    "extrn    __imp_RtlUnwind:dword\n",
    "extrn    __imp_memcmp:dword\n",
    "```\n",
    "\n",
    "The prefix `__imp_` and data type `dword` should also be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apiRegex = re.compile(r\"extrn\\s+(?:__imp_)?([^\\s:]+)\", flags=re.IGNORECASE)\n",
    "set(apiRegex.findall(ReadData.readAsmAsString(TRAIN_LABELS.index[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSystemCallSequence(fileID):\n",
    "    asm = ReadData.readAsmAsString(fileID)\n",
    "    apiCalls = set(apiRegex.findall(asm))\n",
    "    functionCalls = functionCallsRegex.findall(asm)\n",
    "    systemCalls = [i for i in functionCalls if i in apiCalls]\n",
    "    return \" \".join(systemCalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractSystemCallSequence(TRAIN_LABELS.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apiNgramsVectorizer, apiNgrams = extractNGrams(TRAIN_LABELS.index, extractSystemCallSequence, 4)\n",
    "joblib.dump(apiNgramsVectorizer, OBJECT_DUMP_PATH + \"api_4gram.joblib\")\n",
    "apiNgrams.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "#### Calculate the sum of columns and get the columns with the highest frequency, remaining the top 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnSum = apiNgrams.sum(axis=0).A1\n",
    "topFrequencyFeatureIndex = np.argsort(columnSum)[::-1][:5000]\n",
    "columnSum[topFrequencyFeatureIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz(OBJECT_DUMP_PATH +\"api_4grams.npz\", apiNgrams[:, topFrequencyFeatureIndex])\n",
    "List(\"api_4gramNames.txt\").save(topFrequencyFeatureIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature # 4 => OP-CODE 4 Grams\n",
    "\n",
    "The operation code is disassembly instructions such as `mov`, `jmp` and `push`.\n",
    "\n",
    "```\n",
    "6A 00               push    0\n",
    "8B 4C 24 04         mov     ecx, [esp+4]\n",
    "CC CC CC CC CC+     db      17h dup(0CCh)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opcodeRegex = re.compile(r\"\\s[\\dA-F]{2}(?:\\+)?\\s+([a-z]+)\\s\")\n",
    "opcodeRegex.findall(\"8B 4C 24 04     mov  ecx, [esp+4]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractOpcodeSequence(fileID):\n",
    "    opcodes = []\n",
    "    for line in ReadData.readAsmAsList(fileID):\n",
    "        for opcode in opcodeRegex.findall(line):\n",
    "            opcodes.append(opcode)\n",
    "    return \" \".join(opcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractOpcodeSequence(TRAIN_LABELS.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opcodeNgramsVectorizer, opcodeNgrams = extractNGrams(TRAIN_LABELS.index, extractOpcodeSequence, 4)\n",
    "joblib.dump(apiNgramsVectorizer, OBJECT_DUMP_PATH + \"opcode_4gram.joblib\")\n",
    "opcodeNgrams.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "#### Calculate the sum of columns and get the columns with the highest frequency, remaining the top 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnSum = opcodeNgrams.sum(axis=0).A1\n",
    "topFrequencyFeatureIndex = np.argsort(columnSum)[::-1][:5000]\n",
    "columnSum[topFrequencyFeatureIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz(OBJECT_DUMP_PATH +\"opcode_4grams.npz\", apiNgrams[:, topFrequencyFeatureIndex])\n",
    "List(\"opcode_4gramNames.txt\").save(topFrequencyFeatureIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature # 5 => Content Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractContentComplexity(fileIDs):\n",
    "    df = pd.DataFrame(columns=[\"ID\", \"Asm-Len\", \"Zip-Asm-Len\", \"Asm-Zip-Ratio\", \"Byte-Len\", \"Zip-Byte-Len\", \"Byte-Zip-Ratio\"], dtype=float).set_index(\"ID\")\n",
    "    for id in tqdm(fileIDs):\n",
    "        asm = ReadData.readAsmAsString(id).encode(\"utf-8\")\n",
    "        bytes = ReadData.readBytes(id)\n",
    "        bytes = \" \".join([str(byte) for byte in bytes]).encode(\"utf-8\")\n",
    "        df.at[id, \"Asm-Len\"] = len(asm)\n",
    "        df.at[id, \"Zip-Asm-Len\"] = len(zlib.compress(asm))\n",
    "        df.at[id, \"Byte-Len\"] = len(bytes)\n",
    "        df.at[id, \"Zip-Byte-Len\"] = len(zlib.compress(bytes))\n",
    "    df[[\"Asm-Len\", \"Zip-Asm-Len\", \"Byte-Len\", \"Zip-Byte-Len\"]] = df[[\"Asm-Len\", \"Zip-Asm-Len\", \"Byte-Len\", \"Zip-Byte-Len\"]].astype(int)\n",
    "    df[\"Asm-Zip-Ratio\"] = (df[\"Asm-Len\"] / df[\"Zip-Asm-Len\"]).round(5)\n",
    "    df[\"Byte-Zip-Ratio\"] = (df[\"Byte-Len\"] / df[\"Zip-Byte-Len\"]).round(5)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexity = extractContentComplexity(TRAIN_LABELS.index)\n",
    "CSVFeatures(\"contentComplexity.csv\").save(complexity)\n",
    "complexity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "xLimit = (0, complexity[\"Asm-Zip-Ratio\"].mean() + complexity[\"Asm-Zip-Ratio\"].std() * 3)\n",
    "ax = sns.boxplot(y=\"Class\", x=complexity, data=complexity.assign(Class=TRAIN_LABELS[\"Class\"]))\n",
    "ax.set(xlim=xLimit, xlabel=\"Disassembly Compression Ratio\", title=\"Disassembly Compression Ratio\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "xLimit = (0, complexity[\"Byte-Zip-Ratio\"].mean() + complexity[\"Byte-Zip-Ratio\"].std() * 3)\n",
    "ax = sns.boxplot(y=\"Class\", x=complexity, data=complexity.assign(Class=TRAIN_LABELS[\"Class\"]))\n",
    "ax.set(xlim=xLimit, xlabel=\"Machine Code Compression Ratio\", title=\"Machine Code Compression Ratio\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature # 6 => Import Libraries\n",
    "\n",
    "Usually, dependent libraries are imported by the Import Table. In a `.asm` file, their patterns are:\n",
    "\n",
    "```\n",
    "Imports from KERNEL32.dll\n",
    "Imports from java.dll\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractLibrarySet(fileID):\n",
    "    libraryRegex = re.compile(r\"Imports\\s+from\\s+(.+).dll\", flags=re.IGNORECASE)\n",
    "    asm = ReadData.readAsmAsString(fileID)\n",
    "    return set([i.lower() for i in set(libraryRegex.findall(asm))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractClassLibrarySet(type):\n",
    "    samples = [i for i in TRAIN_LABELS.index if TRAIN_LABELS.at[i, \"Class\"] == type]\n",
    "    libs = set()\n",
    "    for i in samples:\n",
    "        libs.update(extractLibrarySet(i))\n",
    "    return libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractLibrarySet(TRAIN_LABELS.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractLibrarySequence(fileID):\n",
    "    libraries = extractLibrarySet(fileID)\n",
    "    return \" \".join(list(libraries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractLibrarySequence(TRAIN_LABELS.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libraryNgramsVectorizer, libraryNgrams = extractNGrams(TRAIN_LABELS.index, extractLibrarySequence, 1)\n",
    "joblib.dump(libraryNgramsVectorizer, OBJECT_DUMP_PATH + \"library_4gram.joblib\")\n",
    "libraryNgrams = csr_matrix(libraryNgrams, dtype=np.int16)\n",
    "libraryNgrams.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "#### Calculate the sum of columns and get the columns with the highest frequency, remaining the top 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnSum = libraryNgrams.sum(axis=0).A1\n",
    "topFrequencyFeatureIndex = np.argsort(columnSum)[::-1][:300]\n",
    "columnSum[topFrequencyFeatureIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz(OBJECT_DUMP_PATH + \"library_1grams.npz\", libraryNgrams[:, topFrequencyFeatureIndex])\n",
    "\n",
    "List(\"library_1gramNames.txt\").save([libraryNgramsVectorizer.get_feature_names()[i] for i in topFrequencyFeatureIndex])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malwareCuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
